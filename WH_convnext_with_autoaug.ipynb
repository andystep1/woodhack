{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WH_convnext_with_autoaug.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4cbe479ed2ba45f5bfb87a516a3694d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7515e866ea04a5c994724ef5cb28972",
              "IPY_MODEL_f9b6c6c51c8d4a22a308249d68489bfc",
              "IPY_MODEL_96fccd8f62364a5ca87fcab4f5708b52"
            ],
            "layout": "IPY_MODEL_7d899134839743a796ef504ebb296441"
          }
        },
        "b7515e866ea04a5c994724ef5cb28972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eff3e7f816194a80b4235e6debc46028",
            "placeholder": "​",
            "style": "IPY_MODEL_7a25d2f468e4462eaf51e7ac17fe1afb",
            "value": "100%"
          }
        },
        "f9b6c6c51c8d4a22a308249d68489bfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_414c04fe8e6140b6abf7f1252975fa66",
            "max": 791189585,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_05413b469fa940d1ae64364740202fc5",
            "value": 791189585
          }
        },
        "96fccd8f62364a5ca87fcab4f5708b52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbb22e3567e944ff9fe2d8bd98cad3d6",
            "placeholder": "​",
            "style": "IPY_MODEL_8736ab80e6904ca394456de4900074c4",
            "value": " 755M/755M [00:10&lt;00:00, 96.0MB/s]"
          }
        },
        "7d899134839743a796ef504ebb296441": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eff3e7f816194a80b4235e6debc46028": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a25d2f468e4462eaf51e7ac17fe1afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "414c04fe8e6140b6abf7f1252975fa66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05413b469fa940d1ae64364740202fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbb22e3567e944ff9fe2d8bd98cad3d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8736ab80e6904ca394456de4900074c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Для начала обновитесь до torch-1.11.0 torchvision-0.12.0"
      ],
      "metadata": {
        "id": "-CGVNOzepM3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJFwQarhX8GR",
        "outputId": "ffe960d2-da54-4b7e-e931-0f01613aa8da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Collecting torch\n",
            "  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 750.6 MB 11 kB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.0 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.11.0 torchvision-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем все, что нужно"
      ],
      "metadata": {
        "id": "Njjbs50bl8JS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ykd7LPLA77vu"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageFile\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем папочки, в которые будем делить картинки"
      ],
      "metadata": {
        "id": "zf8YNPABmEUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = os.listdir('/content/drive/MyDrive/train')\n",
        "os.mkdir('/content/images')\n",
        "os.mkdir('/content/images/train_images')\n",
        "os.mkdir('/content/images/valid_images')\n",
        "for i in range(len(class_names)):\n",
        "  os.mkdir(f'/content/images/train_images/{class_names[i]}')\n",
        "  os.mkdir(f'/content/images/valid_images/{class_names[i]}')"
      ],
      "metadata": {
        "id": "rilnhl6D784H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делим картинки, каждую пятую в валидационный сет"
      ],
      "metadata": {
        "id": "m8HL_hRAmHXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_1 = '/content/drive/MyDrive/train'\n",
        "path_2 = '/content/images'\n",
        "for class_ in class_names:\n",
        "  for i, name_img in enumerate(os.listdir(f'/content/drive/MyDrive/train/{class_}')):\n",
        "    if i % 5 == 0:\n",
        "      shutil.copy(f'{path_1}/{class_}/{name_img}', f'{path_2}/valid_images/{class_}/{name_img}')\n",
        "    else:\n",
        "      shutil.copy(f'{path_1}/{class_}/{name_img}', f'{path_2}/train_images/{class_}/{name_img}')\n",
        "      \n",
        "os.rename(\"/content/images/train_images/drova\", \"/content/images/train_images/0\")\n",
        "os.rename(\"/content/images/valid_images/drova\", \"/content/images/valid_images/0\")"
      ],
      "metadata": {
        "id": "52odfRvz786l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "делаем даталоадеры"
      ],
      "metadata": {
        "id": "k766xkH3mOWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_transforms = transforms.Compose([\n",
        "    transforms.Resize((64,64)), \n",
        "    transforms.AutoAugment(),   \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "    ])"
      ],
      "metadata": {
        "id": "xdPF6KYR789m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/images'\n",
        "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          transform=img_transforms)\n",
        "                  for x in ['train_images', 'valid_images']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train_images', 'valid_images']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train_images', 'valid_images']}\n",
        "class_names = image_datasets['train_images'].classes"
      ],
      "metadata": {
        "id": "_wWlC87q79AH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Берем предобученную ConvNext и заменяем выходной слой на слой с тремя выходами"
      ],
      "metadata": {
        "id": "03S28jhYmZGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convnext = models.convnext_large(pretrained=True)"
      ],
      "metadata": {
        "id": "fPWK-Yb379CW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "4cbe479ed2ba45f5bfb87a516a3694d5",
            "b7515e866ea04a5c994724ef5cb28972",
            "f9b6c6c51c8d4a22a308249d68489bfc",
            "96fccd8f62364a5ca87fcab4f5708b52",
            "7d899134839743a796ef504ebb296441",
            "eff3e7f816194a80b4235e6debc46028",
            "7a25d2f468e4462eaf51e7ac17fe1afb",
            "414c04fe8e6140b6abf7f1252975fa66",
            "05413b469fa940d1ae64364740202fc5",
            "fbb22e3567e944ff9fe2d8bd98cad3d6",
            "8736ab80e6904ca394456de4900074c4"
          ]
        },
        "outputId": "c7b0d6e5-63c4-4266-8635-9f410da14509"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/convnext_large-ea097f82.pth\" to /root/.cache/torch/hub/checkpoints/convnext_large-ea097f82.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/755M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4cbe479ed2ba45f5bfb87a516a3694d5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in convnext.named_parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "4LNsKRBF79Ex"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convnext.classifier[2] = nn.Linear(in_features=1536, out_features=3, bias=True)"
      ],
      "metadata": {
        "id": "AiVyT3Stdbl4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объявляем оптимизатор, функцию потерь и пишем функцию обучения"
      ],
      "metadata": {
        "id": "sylHEl9ImhLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.NAdam(convnext.parameters(), lr=0.001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "yd1DVYhM79Ka"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20):\n",
        "    best_metrics = 0\n",
        "    for epoch in range(epochs):\n",
        "        training_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        model.train()\n",
        "        for i, batch in enumerate(tqdm(train_loader)):\n",
        "            optimizer.zero_grad()\n",
        "            inputs, targets = batch\n",
        "            output = model(inputs)\n",
        "            loss = loss_fn(output, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss += loss.data.item() * inputs.size(0)\n",
        "        training_loss /= len(train_loader.dataset)\n",
        "\n",
        "        model.eval()\n",
        "        num_correct = 0 \n",
        "        num_examples = 0\n",
        "        metrics = 0\n",
        "        extra_metric = 0\n",
        "        for i, batch in enumerate(tqdm(val_loader)):\n",
        "            inputs, targets = batch\n",
        "            output = model(inputs)\n",
        "            loss = loss_fn(output,targets) \n",
        "            valid_loss += loss.data.item() * inputs.size(0)\n",
        "              \n",
        "            y_pred = torch.max(F.softmax(output, dim=1), dim=1)[1].numpy()\n",
        "            metrics += accuracy_score(targets.numpy(), y_pred)\n",
        "            extra_metric += f1_score(targets.numpy(), y_pred, average='macro')\n",
        "        valid_loss /= len(val_loader.dataset)\n",
        "        metrics /= len(val_loader)\n",
        "        extra_metric /= len(val_loader)\n",
        "\n",
        "        if metrics > best_metrics:\n",
        "          print('New best model with test accuracy:', metrics)\n",
        "          torch.save(model, \"/content/convnext_weight\")\n",
        "          best_metrics = metrics\n",
        "\n",
        "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}, f1_score = {:.2f}'.format(epoch, training_loss,\n",
        "        valid_loss, metrics, extra_metric))\n"
      ],
      "metadata": {
        "id": "Au02Y4jy79L8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Учим"
      ],
      "metadata": {
        "id": "G5YPI6KMmoch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(convnext, optimizer, loss_fn, dataloaders['train_images'], dataloaders['valid_images'], epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OQbSE5N79OQ",
        "outputId": "53b4facf-f352-4526-9f87-428e71e3405e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 116/116 [01:48<00:00,  1.07it/s]\n",
            "100%|██████████| 29/29 [00:28<00:00,  1.02it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model with test accuracy: 0.9310344827586207\n",
            "Epoch: 0, Training Loss: 0.19, Validation Loss: 0.18, accuracy = 0.93, f1_score = 0.91\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 116/116 [01:53<00:00,  1.02it/s]\n",
            "100%|██████████| 29/29 [00:29<00:00,  1.02s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model with test accuracy: 0.9396551724137931\n",
            "Epoch: 1, Training Loss: 0.21, Validation Loss: 0.19, accuracy = 0.94, f1_score = 0.90\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 116/116 [01:49<00:00,  1.06it/s]\n",
            "100%|██████████| 29/29 [00:27<00:00,  1.07it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2, Training Loss: 0.16, Validation Loss: 0.26, accuracy = 0.89, f1_score = 0.80\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 116/116 [01:48<00:00,  1.07it/s]\n",
            "100%|██████████| 29/29 [00:27<00:00,  1.07it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3, Training Loss: 0.17, Validation Loss: 0.24, accuracy = 0.89, f1_score = 0.86\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 116/116 [01:48<00:00,  1.07it/s]\n",
            "100%|██████████| 29/29 [00:27<00:00,  1.04it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4, Training Loss: 0.17, Validation Loss: 0.18, accuracy = 0.91, f1_score = 0.87\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 116/116 [01:49<00:00,  1.06it/s]\n",
            "100%|██████████| 29/29 [00:28<00:00,  1.02it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5, Training Loss: 0.23, Validation Loss: 0.22, accuracy = 0.90, f1_score = 0.86\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 116/116 [01:48<00:00,  1.07it/s]\n",
            "100%|██████████| 29/29 [00:26<00:00,  1.08it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best model with test accuracy: 0.9482758620689655\n",
            "Epoch: 6, Training Loss: 0.21, Validation Loss: 0.16, accuracy = 0.95, f1_score = 0.93\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 116/116 [01:49<00:00,  1.06it/s]\n",
            "100%|██████████| 29/29 [00:26<00:00,  1.08it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7, Training Loss: 0.20, Validation Loss: 0.27, accuracy = 0.91, f1_score = 0.85\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 116/116 [01:48<00:00,  1.07it/s]\n",
            "100%|██████████| 29/29 [00:28<00:00,  1.01it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8, Training Loss: 0.17, Validation Loss: 0.20, accuracy = 0.91, f1_score = 0.86\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 116/116 [01:48<00:00,  1.07it/s]\n",
            "100%|██████████| 29/29 [00:26<00:00,  1.08it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9, Training Loss: 0.18, Validation Loss: 0.17, accuracy = 0.94, f1_score = 0.92\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем лучшие веса"
      ],
      "metadata": {
        "id": "I-HFQDPQm2Lr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convnext_custom = torch.load('/content/convnext_weight')"
      ],
      "metadata": {
        "id": "HmXh2mJ5jomh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объявляем трансформ без автоаугментации и используем его в предсказании"
      ],
      "metadata": {
        "id": "0teBWPHKm56j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((64,64)),    \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "    ])"
      ],
      "metadata": {
        "id": "__CPIFvnde8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_test = '/content/drive/MyDrive/wood/test'\n",
        "id = []\n",
        "predict_class = []\n",
        "convnext_custom.eval()\n",
        "for img in os.listdir(path_test):\n",
        "  id.append(int(img.split('.')[0]))\n",
        "\n",
        "  img_test = Image.open(f'{path_test}/{img}')\n",
        "  img_test = test_transforms(img_test)\n",
        "  img_test = torch.unsqueeze(img_test, 0)\n",
        "  \n",
        "  prediction = F.softmax(convnext_custom(img_test))\n",
        "  prediction = prediction.argmax()\n",
        "  predict_class.append(class_names[prediction])"
      ],
      "metadata": {
        "id": "8czZzAuU79UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "сохраняем для сабмита"
      ],
      "metadata": {
        "id": "HuA6oNwMnAqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({'id': id, 'class': predict_class}).sort_values(by='id').to_csv('autoaugmented60.csv', index=False)"
      ],
      "metadata": {
        "id": "ghBJz7mu79ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/convnext_weight /content/drive/MyDrive/woodhack/convnext_weight"
      ],
      "metadata": {
        "id": "yHXx3PmP0dYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/autoaugmented60.csv /content/drive/MyDrive/woodhack/convnext_weight/autoaugmented.csv"
      ],
      "metadata": {
        "id": "vsHGwTLDyevN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ojpOlIpNy93n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}